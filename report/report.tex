\documentclass[a4paper,titlepage]{article}
\usepackage[english]{babel}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}

\usepackage[linesnumbered]{algorithm2e}
\DontPrintSemicolon
\SetAlCapSkip{1em}

\begin{document}
\begin{titlepage}
	\centering
	{\scshape\LARGE Utrecht University \par}
	\vspace{1cm}
	{\scshape\Large Experimentation Project \par}
	\vspace{1.5cm}
	{\huge\bfseries Experimental Analysis Of The Current State Of GPUs For Database Query Processing\par}
	\vspace{2cm}
	
	{\Large\itshape Patrick Kostjens \par}
	\href{mailto:p.a.r.kostjens@students.uu.nl}{p.a.r.kostjens@students.uu.nl}
	\vfill
	
	supervised by\par
	Drs.~Hans Philippi
	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}

\begin{abstract}

\textbf{Keywords:} query processing, graphics processing unit, relational database, parallel processing, CUDA
\end{abstract}

\section{Introduction}
Over the past years or even decades a lot of work has been done on database query processing. Most of this work has focused on query processing using CPUs. When developing algorithms for CPUs, algorithms are developed for a single or a few fast cores. %TODO citations

Another processor that is present in a computer is the Graphics Processing Unit (GPU). A GPU has a very different architecture. It has a lot of cores (hundreds or even thousands), but each core is a lot slower than a single CPU core. Originally, GPUs were mostly used for, for example, games or the rendering of images or videos, but in 2002, Thompson et al. \cite{thompson2002} published a paper on using GPUs for general purpose computing.

From that point forward, a lot of work has been done on database query processing using GPUs as well. We will discuss this work in some more detail in section \ref{sec:related-work}. Some impressive results are in this previous work. However, a lot still has to be done as well. First of all, most of the previous work focuses on a specific aspect of query processing, like joining or filtering. While results in such a specific aspect are definitely useful, those results need to be combined in a single implementation to see how a GPU would perform when processing an actual query instead of only a part of it.

An extra challenge in this area is the fact that the comparison between CPU query processing and GPU query processing may have different results in different years because CPUs and GPUs may be developed at different speeds. This means that initially it may be more efficient to perform a certain operation on a CPU because the GPU version of the algorithm is slower, while a couple of years later this might be the other way around when using the same algorithms.

Last year I also created a general overview of the possibilities \cite{kostjens2015}. However, in this experimental analysis of the current state of database query processing on GPUs, I will try to implement and combine some query operators on GPUs and CPUs to compare them to each other. This way, I hope to find how useful GPUs currently are compared to CPUs for database query processing. I will also try to make a comparison with the results found in the original work and I will try to combine the operations to find how useful the GPU is for processing more complete queries instead of only parts of them. This work will therefore be more detailed than my overview from last year and it will contain recent results for the discussed algorithms.

%TODO memory strategies

I will first discuss some related work in section \ref{sec:related-work}. Some of this work is also used to implement the GPU operations. Next, we will look at an overview of the current possibilities and some choices that were made in section \ref{sec:overview}. After that, we will discuss the implemented algorithms in section \ref{sec:implementation}. We will then look at the results in section \ref{sec:results}. Next, we will have a discussion about the results and the general state of GPUs for query processing in section \ref{sec:discussion}. Finally, we will conclude with some final remarks and a small summary in section \ref{sec:conclusion}.

\section{Related work}
\label{sec:related-work}
Since the paper by Thompson et al. in 2002 on general purpose computing using GPUs \cite{thompson2002}, a lot of research has been done on the topic of general purpose GPU's. One of the more specific topics in this area is that of database query processing using GPUs. Quite some research has been done in this area as well \cite{bakkum2010}, \cite{fang2007}, \cite{kaldeway2010}. 

So far, a lot of papers have focused on specific aspects of query processing. For example, He et al. \cite{he2008} and Kaldeway et al. \cite{kaldeway2010} researched joins, while, for example, Bakkum et al. \cite{bakkum2010} and Govinderaju et al. \cite{govindaraju2004} looked at algebraic selection (filtering).  Fang et al. \cite{fang2007} even did some work on combining the processing power of CPUs and GPUs.

Although quite a bit of work has been done on query processing using GPUs, a lot more research has been done on query processing using CPUs. This work also started a lot earlier. For example, in 1984, Bratbergsengen published a paper on the implementation of several relational algebra operations \cite{bratbergsengen1984}. These algebra operations are fundamental parts of database query processing.

\section{Overview}
\label{sec:overview}
Before being able to start the implementation of any actual GPU algorithms, a few choices had to be made. For example, which GPU to use. Other important choices include the development environment, and source of test data. Since I already had a capable, moderately recent GPU available I chose to use that one. This GPU is a GTX 660 Ti from Nvidia \cite{gtx660ti}, which was released in August 2012. This GPU is accompanied by Intel's i5-750 CPU from late 2009 \cite{i5-750}.

As the development environment, Visual Studio 2013 is used to work with CUDA \cite{CUDA} and C++. CUDA is NVIDIA's own parallel computing platform that I will be using to utilize the GPU. The well known TPC-H benchmark will be used as the source for test data \cite{tpc-h}.

Some differences between CPUs and GPUs exist that are very important for the development of query processing algorithms for GPUs. First of all, as mentioned already, GPUs have a lot of relatively slow cores, while CPUs have a few fast cores. This makes the nature of the algorithms for both very different since GPUs can only work with highly parallel algorithms.

Another important difference is related to the parallelism. While atomic operations for GPUs do exist, using them can significantly slow down your algorithm. If, for example, every core needs to get a memory address to write its results to, you could get an address and increase it atomically on a CPU. However, on a GPU, when hundreds or even thousands of cores are doing these atomic operations on the same memory address, this can cause significant waiting times, thereby slowing down the algorithm. This means alternative solutions for this problem, which we will discuss later, will have to be developed.

A third important difference that can influence GPU algorithms is the fact that a GPU has its own memory. It cannot use the memory of its host directly. This means that all the data that the GPU requires needs to be transferred to the GPU before it can be used. Different strategies to do this exist. We will discuss them in more detail in section \ref{sec:gpu-implementation}.

These two differences are the most important for the design of algorithms for GPUs. A more extensive overview of the differences between CPUs and GPUs can be found in my overview from last year \cite{kostjens2015}.

%TODO explain CUDA's thread/block structure

\section{Implementation}
\label{sec:implementation}
The original idea was to implement all the important algebraic operations for query processing and to combine them as well. However, it turned out to be too much work to do all of this. Eventually, I implemented non indexed and indexed filtering for CPUs, as well as hash join and sort-merge join. For GPUs, non indexed filtering was implemented using Unified Memory and both synchronously and asynchronously copying the results manually. Sort-merge join was also implemented for GPUs.

Simple projection (meaning column filtering) could have been implemented relatively easy as well for GPUs, but since this would not include any real computations, I did not do that. However, if there would have been time to implement projections including aggregations, the results could have been interesting.

\subsection{CPU}
Since a performance comparison is needed to evaluate the results for the GPU processing, the same or similar operations were also implemented on a CPU. All CPU algorithms were implemented for a single core.

\subsubsection{Non-indexed filtering}
The most simple algorithm is the non-indexed filtering algorithm. This algorithm simply iterates over all the records and uses a given predicate to determine whether the record will be in the result or not.

\subsubsection{Indexed filtering}
A filtering algorithm that is a bit more complex is the indexed filtering algorithm. I implemented a B-tree to serve as the index \cite{comer1979}. First, the B-tree is built for a predetermined column or set of columns. Next, this tree is searched for elements with a certain value for the indexed columns. The result can consist of zero or more values.

\subsubsection{Hash join}
One of the CPU join algorithms I implemented is a simple hash join algorithm \cite{dewitt1985}. This algorithm works by creating two hash tables, each containing the records for one of the join relationships. The join key is used for hashing. The join result is computed by taking the cartesian product of every pair of partitions that has the same key.

\subsubsection{Sort-merge join}
The other CPU join algorithm that is implemented is the sort-merge join algorithm \cite{blasgen1977}. This algorithm works by first sorting both relationships. It then iterates through both of them and adds a record to the result for every matching pair of records that is encountered in the input relationships.

\subsection{GPU}
\label{sec:gpu-implementation}
We will now discuss the implementations of the GPU algorithms. We will start by looking at the different memory strategies that can be used. We will experiment with the different memory strategies using a simple non-indexed filtering algorithm. After that we will also look at the GPU implementation of the sort-merge join algorithm.

\subsubsection{Non-indexed filter with synchronous copying}
The most simple way to transfer all the data from the host memory to the GPU is by synchronously copying all the data at once. In this case, the actual processing of the data will not start until all the data has been copied. The GPU creates a list of booleans indicating for each record whether it matches the predefined filter. This is done using a filter kernel that is executed for every row. This means that for every record, a thread will be created to determine whether the record should be in the result or not. When the GPU completes its processing, this list of booleans is copied back to the host. The CPU is then used to create a list containing only the results that passed the filter.

This might sound like an inefficient way to do the filtering, but using atomic operations on GPUs can be very slow as we mentioned before. If we want to create the actual resulting list on the GPU, we would need to add a record to a list every time we find a record that matches our filter. However, there is no easy way to do this without causing either a lot of waits from the atomic operations or memory conflicts because multiple threads are writing to the same location in the list. Therefore, we create a list of booleans indicating for each row whether it matches the filter or not so that we can use that to build the actual list on the CPU.

Although the actual filtering might be very fast, it can be expected that the entire step is very slow due to the big amount of data that needs to be transferred between the host memory and the GPU memory. It is therefore likely that this step will become relatively more efficient when the filtering condition becomes more computationally expensive, since the relative time that is spent on operations that can be parallelized is bigger in that case.

\subsubsection{Non-indexed filter with asynchronous copying}
This algorithm is mostly the same as the algorithm described above. Therefore, the same considerations can also be made. However, an important difference with the algorithm above is the memory strategy. Instead of doing the copying and processing sequentially, we can also do them partially at the same time.

We can do this by utilizing CUDA's so called streams. The idea is to divide the input data over the different streams. This means that we do multiple smaller copy operations instead of a single big copy operation. After the completion of a copy operation for a stream, we start the processing of the data for that stream. We can then start copying the data for the next stream, while the GPU is still processing the previous stream. This idea has been described more extensively by Mark Harris on NVIDIA's developer blog \cite{cuda-overlap-streams}.

It is important to take the capabilities of the GPU into account when using this feature. It depends on the GPU if and how many copy and/or execution kernels can be used at the same time. Using this memory strategy on GPUs that do not support concurrent copying and execution can actually decrease the performance. The GPU used for the experiment does support this feature and can therefore benefit from this memory strategy.

\subsubsection{Non-indexed filtering with Unified Memory}
This is the third version of the simple filtering algorithm which was described above as well. The same considerations also apply for this memory strategy. The difference in this version is the use of Unified Memory instead of explicitly copying the input data \cite{harris2013}.

The goal of Unified Memory is to simplify memory management. It does this by making a pool of memory available to both the GPU and the CPU as if they are actually using the same memory. What actually happens is that all the memory in that pool is automatically copied between the CPU and GPU as needed. This makes development easier since the explicit copying of data is no longer necessary. It also has the potential to improve the performance of your application. Unified Memory only copies data that is needed at that moment. That means that not all the data is copied if only parts are needed.

For our algorithm, this means that we allocate our input and result vectors in the shared pool of Unified Memory. We can then use all the data on both the CPU and GPU without explicitly copying it. This made the implementation easier, but four our application, the performance will probably not improve. Since we look at every record for the filtering process, Unified Memory will still have to copy all the data which means that ease of implementation is probably the only advantage in this case.

\subsubsection{Sort-merge join}
The next algorithm we implemented on the GPU is the sort-merge join algorithm. In 2008, He et al. also implemented this algorithm for GPUs \cite{he2008}. Some ideas for the implementation were used. This algorithm was implemented using Nvidia's Thrust library \cite{thrust}. This library contains many basic functions which are important primitives for the implementation of our join algorithm. 

\begin{algorithm}[ht]
 \label{alg:gpu-sort-merge-join}
 \KwIn{A data set $L$ and a data set $R$ that need to be joined and a key selection function for both ($leftKeySelector$ and $rightKeySelector$ respectively).}
 \KwOut{The join result.}
 
 \tcc{Prepare the data by extracting the keys and sorting everything}
 $leftKeys$ = Apply $leftKeySelector$ to every record in $L$\;
 $rightKeys$ = Apply $rightKeySelector$ to every record in $R$\;
 $leftKeys, L$ = Sort $L$ and $leftKeys$ using $leftKeys$ \;
 $rightKeys, R$ = Sort $R$ and $rightKeys$ using $rightKeys$ \;
 \;
 \tcc{Calculate keys and sizes for all the partitions on both sides of the join}
 $leftPartitionKeys, leftPartitionCounts$ = Reduction over $leftKeys$ to get keys and counts of consecutive equal keys for the partitions \;
 $rightPartitionKeys, rightPartitionCounts$ = Reduction over $rightKeys$ to get keys and counts of consecutive equal keys for the partitions \;
 \;
 \tcc{Calculate sizes of cross product of all matching partitions}
 $partitionSizes$ = Compute multiplication of counts ($leftPartitionCounts, rightPartitionCounts$) for each pair of matching keys in $leftPartitionKeys, rightPartitionKeys$ \;
 \;
 \tcc{Compute all the required partition start indexes}
 $startIndexes$ = Compute prefix sum of $partitionSizes$ \;
 $leftStartIndexes$ = Compute prefix sum of $leftPartitionCounts$ \;
 $rightStartIndexes$ = Compute prefix sum of $rightPartitionCounts$ \;
 \;
 \tcc{Compute the actual join result}
 $joinResult$ = Execute partition join kernel for all partitions (see algorithm \ref{alg:partition-join-kernel})\;
 \Return $joinResult$ \;
 
 \caption{GPU sort-merge join}
\end{algorithm}

\begin{algorithm}[ht]
 \label{alg:partition-join-kernel}
 \KwIn{$partitionIndex$, $leftStartIndexes$, $rightStartIndexes$, $resultStartIndexes$, $leftPartitionSizes$, $rightPartitionSizes$, $leftData$, $rightData$, $resultList$}
 \KwOut{The partition join result at the appropriate offset in the $resultList$ paramater.}
 $leftOffset$ = $leftStartIndexes[partitionIndex]$ \;
 $rightOffset$ = $rightStartIndexes[partitionIndex]$ \;
 $resultOffset$ = $resultStartIndexes[partitionIndex]$ \;
 \;
 \For{$i = 0; i < leftPartitionSizes[partitionIndex]; i++$}{
  \For{$j = 0; j < rightPartitionSizes[partitionIndex]; j++$}{
   $resultList[resultOffset]$ = Tuple of $leftData[leftOffset + i]$ and $rightData[rightOffset + j]$ \;
   $resultOffset$ = $resultOffset + 1$\;
  }
 }
 \caption{CUDA kernel to join two matching partitions}
\end{algorithm}

The pseudo code for the algorithm can be found in algorithm \ref{alg:gpu-sort-merge-join} and \ref{alg:partition-join-kernel}. This pseudo code has been simplified so it does not include the memory management that is required to make all the operations work. All the calculations shown in the algorithms are computed on the GPU. However, in the actual implementation, the CPU plays an important role in the memory management. 

The algorithm first calculates a lot of sizes and offsets. These are all required to know where each partition's input needs to be read and where the output of the join results needs to be written beforehand. If we would not calculate beforehand where all the results need to be written, we would not be able to actually produce the result efficiently because either memory conflicts would occur or we would need to slow down the algorithm significantly using atomic operations. We use the partition start indexes for reading because we needed to calculate them already to determine where the results need to be written and it is then far more efficient to reuse them instead of iterating over all records until we find the correct partition.

\section{Results}
\label{sec:results}

\section{Discussion}
\label{sec:discussion}

\subsection{Ease of implementation}

\subsection{Performance}

\subsection{Expectations}

\subsection{Future work}

\section{Conclusion}
\label{sec:conclusion}

\bibliography{references}{}
\bibliographystyle{acm}

\end{document}
